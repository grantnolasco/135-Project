{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "## I. Setting Up Environment\n",
    "1. Setting up Spark Session and Loading Data\n",
    "\n",
    "## II. Preprocessing\n",
    "2. Non-empty reviews\n",
    "\n",
    "3. Selecting 4 styles of beer to classify\n",
    "\n",
    "4. Cleaning numeric fields\n",
    "\n",
    "## III. Exploratory Analysis and Vizualizations\n",
    "5. Summary statistics for each style of beer and for entire dataset\n",
    "\n",
    "## IV. Modeling\n",
    "6. Creating Pipelines and Cross Validators\n",
    "\n",
    "7. Baseline Model for Entire Dataset (Unbalanced Classes)\n",
    "\n",
    "    7.1 CV TF-IDF Parameters\n",
    "    \n",
    "8. Baseline Model for Sampled Dataset (Balanced Classes)\n",
    "\n",
    "    8.1 CV TF-IDF Parameters\n",
    "    \n",
    "    8.2 Choose Best parameters for Baseline ML Models\n",
    "    \n",
    "10. Baseline for Machine Learning Models with Balanced data and TF-IDF Parameters\n",
    "\n",
    "    10.1 Naive Bayes Base Model (Should be same as 8.1)\n",
    "    \n",
    "    10.2 Logistic Regression Base Model\n",
    "    \n",
    "    10.3 Random Forest Base Model\n",
    "    \n",
    "11. CV ML Models\n",
    "\n",
    "    11.1 Best Naive Bayes Parameters\n",
    "    \n",
    "    11.2 Best Logistic Regression Parameters\n",
    "    \n",
    "    11.3 Best Random Forest Parameters\n",
    "\n",
    "## V. Next Steps (might be able to finish)\n",
    "\n",
    "12. Stemming or Lemmatization\n",
    "\n",
    "13. N-Grams\n",
    "\n",
    "## VI. Next Steps (probably won't be able to finish)\n",
    "14. Deep Learning\n",
    "\n",
    "15. Recommendation System\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I . Setting Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Spark Session and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Exploratory_Analysis\") \\\n",
    "    .config(\"spark.executor.memory\", '12g') \\\n",
    "    .config(\"spark.executor.cores\", '5') \\\n",
    "    .config('spark.cores.max', '5') \\\n",
    "    .config('spark.driver.memory', '12g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers = spark.read.format('csv'). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(\"C:/Users/Eri/Documents/PSTAT 135/beers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.format('csv'). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(\"C:/Users/Eri/Documents/PSTAT 135/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- brewery_id: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- style: string (nullable = true)\n",
      " |-- availability: string (nullable = true)\n",
      " |-- abv: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- retired: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- beer_id: integer (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- look: string (nullable = true)\n",
      " |-- smell: string (nullable = true)\n",
      " |-- taste: string (nullable = true)\n",
      " |-- feel: string (nullable = true)\n",
      " |-- overall: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+-----+-------+--------------------+------------+----+--------------------+-------+\n",
      "|    id|                name|brewery_id|state|country|               style|availability| abv|               notes|retired|\n",
      "+------+--------------------+----------+-----+-------+--------------------+------------+----+--------------------+-------+\n",
      "|202522|      Olde Cogitator|      2199|   CA|     US|English Oatmeal S...|    Rotating| 7.3|No notes at this ...|      f|\n",
      "| 82352|Konrads Stout Rus...|     18604| null|     NO|Russian Imperial ...|    Rotating|10.4|No notes at this ...|      f|\n",
      "|214879|      Scottish Right|     44306|   IN|     US|        Scottish Ale|  Year-round|   4|No notes at this ...|      t|\n",
      "|320009|MegaMeow Imperial...|      4378|   WA|     US|American Imperial...|      Winter| 8.7|Every time this year|      f|\n",
      "|246438|     Peaches-N-Cream|     44617|   PA|     US|  American Cream Ale|    Rotating| 5.1|No notes at this ...|      f|\n",
      "+------+--------------------+----------+-----+-------+--------------------+------------+----+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358873"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               style|count|\n",
      "+--------------------+-----+\n",
      "|        American IPA|44719|\n",
      "|American Pale Ale...|22159|\n",
      "|American Imperial...|18336|\n",
      "|      Belgian Saison|18166|\n",
      "|   American Wild Ale|12972|\n",
      "|American Imperial...|11180|\n",
      "|     American Porter|10168|\n",
      "|American Amber / ...| 9748|\n",
      "|      American Stout| 9103|\n",
      "|Fruit and Field Beer| 7729|\n",
      "| American Blonde Ale| 7089|\n",
      "|  American Brown Ale| 7008|\n",
      "|   German Hefeweizen| 6019|\n",
      "|     Belgian Witbier| 5613|\n",
      "|American Pale Whe...| 5266|\n",
      "|     Berliner Weisse| 5036|\n",
      "|      German Pilsner| 4748|\n",
      "|    Belgian Pale Ale| 4523|\n",
      "|Russian Imperial ...| 4426|\n",
      "|English Sweet / M...| 4192|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many beers in each style\n",
    "beers.groupBy('style').count().sort('count', ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "|beer_id|       username|               date|                text|                look|               smell| taste|                feel|          overall|             score|\n",
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "| 271781|   bluejacket74|2017-03-17 00:00:00|   750 ml bottle,...|                   4|                   4|     4|                4.25|                4|              4.03|\n",
      "| 125646|        _dirty_|2017-12-21 00:00:00|                    |                 4.5|                 4.5|   4.5|                 4.5|              4.5|               4.5|\n",
      "| 125646|        CJDUBYA|2017-12-21 00:00:00|                    |                4.75|                4.75|  4.75|                4.75|             4.75|              4.75|\n",
      "| 125646|GratefulBeerGuy|2017-12-20 00:00:00|\"   0% 16 oz can....| bloomin' like a ...| totally unfilter...| thick| all-white clumps...| mellon and mango| grainy earthiness|\n",
      "| 125646|       LukeGude|2017-12-20 00:00:00|   Classic TH NEI...|                4.25|                 4.5|  4.25|                4.25|             4.25|              4.31|\n",
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9073128"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Non Empty Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2987993"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count Non-empty reviews\n",
    "(reviews.filter(reviews['text'] != '\\xa0\\xa0')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_reviews = reviews.filter(reviews['text'] != '\\xa0\\xa0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_reviews = non_empty_reviews.withColumn('text', F.regexp_replace('text', \"\\\\.|\\xa0|!|,|:\", \"\"))\\\n",
    "                                    .withColumn('text', F.trim(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "|beer_id|       username|               date|                text|                look|               smell| taste|                feel|          overall|             score|\n",
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "| 271781|   bluejacket74|2017-03-17 00:00:00|750 ml bottle 201...|                   4|                   4|     4|                4.25|                4|              4.03|\n",
      "| 125646|GratefulBeerGuy|2017-12-20 00:00:00|\" 0% 16 oz can Fu...| bloomin' like a ...| totally unfilter...| thick| all-white clumps...| mellon and mango| grainy earthiness|\n",
      "| 125646|       LukeGude|2017-12-20 00:00:00|Classic TH NEIPA ...|                4.25|                 4.5|  4.25|                4.25|             4.25|              4.31|\n",
      "| 125646|           MFMB|2017-12-16 00:00:00|Pours a creamy op...|                4.75|                 4.5|   4.5|                 4.5|              4.5|              4.52|\n",
      "| 125646|    jngrizzaffi|2017-12-10 00:00:00|Pours a cloudy ye...|                 4.5|                 4.5|   4.5|                4.75|              4.5|              4.53|\n",
      "| 125646|       PDOR1960|2017-12-08 00:00:00|Another great bre...|                 4.5|                 4.5|   4.5|                 4.5|              4.5|               4.5|\n",
      "| 125646|        Lucular|2017-12-04 00:00:00|Pours with a clou...|                4.25|                4.25|  4.25|                4.25|             4.25|              4.25|\n",
      "| 205644|    Brutaltruth|2017-03-29 00:00:00|From the tall boy...|                   4|                3.75|     4|                3.75|                4|              3.92|\n",
      "| 205644|    secondtooth|2016-07-13 00:00:00|Appearance Pours ...|                 3.5|                   4|  4.25|                   4|                4|              4.07|\n",
      "| 150672|          Derek|2016-06-07 00:00:00|Beautiful crystal...|                4.75|                   4|  4.25|                4.25|             4.25|              4.22|\n",
      "+-------+---------------+-------------------+--------------------+--------------------+--------------------+------+--------------------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_empty_reviews.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT beer_id)|\n",
      "+-----------------------+\n",
      "|                 210311|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counts how many beers have non-empty reviews\n",
    "non_empty_reviews.agg(F.countDistinct(\"beer_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beerStyles = beers.select(\"id\",\"style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|               style|\n",
      "+------+--------------------+\n",
      "|202522|English Oatmeal S...|\n",
      "| 82352|Russian Imperial ...|\n",
      "|214879|        Scottish Ale|\n",
      "|320009|American Imperial...|\n",
      "|246438|  American Cream Ale|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beerStyles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beerStyles = beerStyles.withColumnRenamed('id', 'beer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDF = non_empty_reviews.join(beerStyles, \"beer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210294"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainDF.select('beer_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|style                   |count |\n",
      "+------------------------+------+\n",
      "|American IPA            |301774|\n",
      "|American Imperial IPA   |212697|\n",
      "|American Imperial Stout |150160|\n",
      "|American Pale Ale (APA) |126489|\n",
      "|Belgian Saison          |91000 |\n",
      "|Russian Imperial Stout  |86117 |\n",
      "|American Porter         |71189 |\n",
      "|American Wild Ale       |63393 |\n",
      "|American Amber / Red Ale|62818 |\n",
      "|Fruit and Field Beer    |58342 |\n",
      "|Belgian Strong Dark Ale |53097 |\n",
      "|Belgian Witbier         |46545 |\n",
      "|Belgian Strong Pale Ale |45732 |\n",
      "|Belgian Tripel          |45686 |\n",
      "|American Brown Ale      |44774 |\n",
      "|American Strong Ale     |43575 |\n",
      "|German Hefeweizen       |42930 |\n",
      "|American Stout          |41879 |\n",
      "|American Barleywine     |40873 |\n",
      "|American Adjunct Lager  |39404 |\n",
      "+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counts how many reviews for each style of beer\n",
    "mainDF.groupBy('style').count().sort('count', ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|style                   |count|\n",
      "+------------------------+-----+\n",
      "|American IPA            |24380|\n",
      "|American Pale Ale (APA) |12216|\n",
      "|American Imperial IPA   |11517|\n",
      "|Belgian Saison          |9744 |\n",
      "|American Wild Ale       |7390 |\n",
      "|American Imperial Stout |7016 |\n",
      "|American Porter         |5889 |\n",
      "|American Amber / Red Ale|5573 |\n",
      "|American Stout          |4782 |\n",
      "|Fruit and Field Beer    |4471 |\n",
      "|American Brown Ale      |3682 |\n",
      "|German Hefeweizen       |3395 |\n",
      "|American Blonde Ale     |3329 |\n",
      "|Belgian Witbier         |3036 |\n",
      "|German Pilsner          |3024 |\n",
      "|Russian Imperial Stout  |2946 |\n",
      "|American Pale Wheat Ale |2793 |\n",
      "|Belgian Pale Ale        |2630 |\n",
      "|Berliner Weisse         |2586 |\n",
      "|English Bitter          |2545 |\n",
      "+------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counts how many unique beers of that style is reviewed\n",
    "mainDF.select('beer_id', 'style').distinct().groupBy('style').count().sort('count', ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see American Imperial Stout has the 3rd most reviews but only 6th most unique beers. This means that there are a few beers that are American Imperial Stouts with many reviews. We will choose Belgian Saison for classification because it has the 5th most reviews and the 4th most unique beers that have been reviewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting Targets, Top 4 most reviewed styles (with most unique beers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "styleTargets = ['American IP|A', 'American Pale Ale (APA)', 'American Imperial IPA', 'Belgian Saison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731960"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainDF.filter(mainDF['style'].isin(styleTargets)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainStyles = mainDF.filter(mainDF['style'].isin(styleTargets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+\n",
      "|style                  |count |\n",
      "+-----------------------+------+\n",
      "|American IPA           |301774|\n",
      "|American Imperial IPA  |212697|\n",
      "|American Pale Ale (APA)|126489|\n",
      "|Belgian Saison         |91000 |\n",
      "+-----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainStyles.groupBy('style').count().sort('count', ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|style                  |count|\n",
      "+-----------------------+-----+\n",
      "|American IPA           |24380|\n",
      "|American Pale Ale (APA)|12216|\n",
      "|American Imperial IPA  |11517|\n",
      "|Belgian Saison         |9744 |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainStyles.select('beer_id', 'style').distinct().groupBy('style').count().sort('count', ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- beer_id: integer (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- look: string (nullable = true)\n",
      " |-- smell: string (nullable = true)\n",
      " |-- taste: string (nullable = true)\n",
      " |-- feel: string (nullable = true)\n",
      " |-- overall: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- style: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainStyles.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should Remove rows with NA's and Text in Numerical Columns\n",
    "model_df = mainStyles.filter(F.col(\"look\").cast(\"int\").isNotNull() == True)\\\n",
    "            .filter(F.col(\"smell\").cast(\"int\").isNotNull() == True)\\\n",
    "            .filter(F.col(\"taste\").cast(\"int\").isNotNull() == True)\\\n",
    "            .filter(F.col(\"feel\").cast(\"int\").isNotNull() == True)\\\n",
    "            .filter(F.col(\"overall\").cast(\"int\").isNotNull() == True)\\\n",
    "            .filter(F.col(\"score\").cast(\"int\").isNotNull() == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast Numerical columns as floats now that strings and na's are removed\n",
    "model_df = model_df.withColumn('look', model_df['look'].cast(\"float\"))\\\n",
    "        .withColumn('smell', model_df['smell'].cast(\"float\"))\\\n",
    "        .withColumn('taste', model_df['taste'].cast(\"float\"))\\\n",
    "        .withColumn('feel', model_df['feel'].cast(\"float\"))\\\n",
    "        .withColumn('overall', model_df['overall'].cast(\"float\"))\\\n",
    "        .withColumn('score', model_df['score'].cast(\"float\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop(\"username\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o268.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 40, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\Eri\\AppData\\Local\\Temp\\blockmgr-be246dbb-1956-4779-98ac-973df732313d\\38\\temp_shuffle_f198667b-89a7-4b45-a01a-06b93552b494 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Eri\\AppData\\Local\\Temp\\blockmgr-be246dbb-1956-4779-98ac-973df732313d\\38\\temp_shuffle_f198667b-89a7-4b45-a01a-06b93552b494 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a11869be5cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Last check for NA's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o268.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 40, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\Eri\\AppData\\Local\\Temp\\blockmgr-be246dbb-1956-4779-98ac-973df732313d\\38\\temp_shuffle_f198667b-89a7-4b45-a01a-06b93552b494 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\Eri\\AppData\\Local\\Temp\\blockmgr-be246dbb-1956-4779-98ac-973df732313d\\38\\temp_shuffle_f198667b-89a7-4b45-a01a-06b93552b494 (The system cannot find the path specified)\r\n\tat java.io.FileOutputStream.open0(Native Method)\r\n\tat java.io.FileOutputStream.open(Unknown Source)\r\n\tat java.io.FileOutputStream.<init>(Unknown Source)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Last check for NA's\n",
    "model_df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in model_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- beer_id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- look: float (nullable = true)\n",
      " |-- smell: float (nullable = true)\n",
      " |-- taste: float (nullable = true)\n",
      " |-- feel: float (nullable = true)\n",
      " |-- overall: float (nullable = true)\n",
      " |-- score: float (nullable = true)\n",
      " |-- style: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Exploratory Analysis and Vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of Means for Each Style and Summary Statistics for Entire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|               style|         avg(look)|        avg(smell)|        avg(taste)|         avg(feel)|      avg(overall)|        avg(score)|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|        American IPA| 3.986425988960963|3.9190130602160207|3.9318579356773973|3.9012531309383935|3.9446924219012685| 3.932736264468442|\n",
      "|American Imperial...| 4.107345298701574|4.0981487760042805|4.1012994856092435| 4.058349711549159|4.0595800643120885| 4.089419074101148|\n",
      "|American Pale Ale...|3.8744497214357474|3.7821011051237554|3.8132181021097815|3.7911087770572656|3.8725111882363685|3.8200986432859327|\n",
      "|      Belgian Saison|3.9922546618708705|3.9455913114461367|3.9475109053340476|3.9248201576490396| 3.955256243463177|3.9501903062389685|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.groupBy('style')\\\n",
    "        .agg(F.mean('look'), F.mean('smell'), F.mean('taste'), F.mean('feel'), F.mean('overall'),\n",
    "            F.mean('score')).show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------+\n",
      "|summary|          beer_id|                text|              look|             smell|             taste|              feel|           overall|            score|         style|\n",
      "+-------+-----------------+--------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------+\n",
      "|  count|           641356|              641356|            641356|            641356|            641356|            641356|            641356|           641356|        641356|\n",
      "|   mean|86016.15441190229|            145855.5| 4.003611878582253|3.9516130666899505|3.9633885080984665| 3.931568037096402|3.9674755517996245|3.961756469417888|          null|\n",
      "| stddev|86104.03033777044|   995375.9829575884|0.4889811619131977|0.5641361351138886|0.5939057486341173|0.5428806486968022|0.5714921030640363|0.508148213541669|          null|\n",
      "|    min|                3|                    |               1.0|               1.0|               1.0|               1.0|               1.0|              1.0|  American IPA|\n",
      "|    max|           372836|￼Really well done...|               5.0|               5.0|               5.0|               5.0|               5.0|              5.0|Belgian Saison|\n",
      "+-------+-----------------+--------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Up Pipelines \n",
    "\n",
    "Note: HashingTF and IDF are updated with best parameters for parts 7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-grams \n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+\n",
      "|beer_id|                text|look|smell|taste|feel|overall|score|               style|\n",
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+\n",
      "|  92616|Drank on 6/6/18 b...| 4.0|  3.0|  3.5| 5.0|    4.0| 3.66|American Imperial...|\n",
      "|  92616|2016 On tap at th...|3.75| 3.25| 3.25|3.25|   3.25| 3.28|American Imperial...|\n",
      "|  92616|22oz bomber picke...| 4.0|  4.0|  4.0| 4.0|   3.75| 3.95|American Imperial...|\n",
      "|  92616|If you are a fan ...| 2.5|  2.0|  1.0| 1.0|    1.5| 1.43|American Imperial...|\n",
      "|  92616|Good iPa great ta...| 3.0| 4.75| 4.75|4.75|   4.75| 4.65|American Imperial...|\n",
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "data1 = tokenizer.transform(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+--------------------+\n",
      "|beer_id|                text|look|smell|taste|feel|overall|score|               style|               words|\n",
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+--------------------+\n",
      "|  92616|Drank on 6/6/18 b...| 4.0|  3.0|  3.5| 5.0|    4.0| 3.66|American Imperial...|[drank, on, 6/6/1...|\n",
      "|  92616|2016 On tap at th...|3.75| 3.25| 3.25|3.25|   3.25| 3.28|American Imperial...|[2016, on, tap, a...|\n",
      "|  92616|22oz bomber picke...| 4.0|  4.0|  4.0| 4.0|   3.75| 3.95|American Imperial...|[22oz, bomber, pi...|\n",
      "|  92616|If you are a fan ...| 2.5|  2.0|  1.0| 1.0|    1.5| 1.43|American Imperial...|[if, you, are, a,...|\n",
      "|  92616|Good iPa great ta...| 3.0| 4.75| 4.75|4.75|   4.75| 4.65|American Imperial...|[good, ipa, great...|\n",
      "+-------+--------------------+----+-----+-----+----+-------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram\n",
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|              bigram|\n",
      "+--------------------+--------------------+\n",
      "|[drank, on, 6/6/1...|[drank on, on 6/6...|\n",
      "|[2016, on, tap, a...|[2016 on, on tap,...|\n",
      "|[22oz, bomber, pi...|[22oz bomber, bom...|\n",
      "|[if, you, are, a,...|[if you, you are,...|\n",
      "|[good, ipa, great...|[good ipa, ipa gr...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigramDataFrame = bigram.transform(data1).select(\"words\", \"bigram\")\n",
    "bigramDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigram\n",
    "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|             trigram|\n",
      "+--------------------+--------------------+\n",
      "|[drank, on, 6/6/1...|[drank on 6/6/18,...|\n",
      "|[2016, on, tap, a...|[2016 on tap, on ...|\n",
      "|[22oz, bomber, pi...|[22oz bomber pick...|\n",
      "|[if, you, are, a,...|[if you are, you ...|\n",
      "|[good, ipa, great...|[good ipa great, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigramDataFrame = trigram.transform(data1).select(\"words\", \"trigram\")\n",
    "trigramDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"words\")\n",
    "stopRem = StopWordsRemover(inputCol = 'words', outputCol = 'filtered')\n",
    "hashingTF = HashingTF(inputCol = \"filtered\", outputCol = \"rawFeatures\", numFeatures = 100000)\n",
    "idf = IDF(inputCol = \"rawFeatures\", outputCol = \"features\", minDocFreq = 100)\n",
    "stringIdx = StringIndexer(inputCol = 'style', outputCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "pipelineNB = Pipeline(stages=[tokenizer, stopRem, hashingTF, idf, stringIdx, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10, regParam=0.01)\n",
    "pipelineLogReg = Pipeline(stages=[tokenizer, stopRem, hashingTF, idf, stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"features\", numTrees = 20, maxDepth = 10, maxBins = 32)\n",
    "pipelineRF = Pipeline(stages = [tokenizer, stopRem, hashingTF, idf, stringIdx, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridTFIDF = ParamGridBuilder()\\\n",
    "    .addGrid(hashingTF.numFeatures, [1000, 10000, 50000, 100000])\\\n",
    "    .addGrid(idf.minDocFreq, [100, 1000, 10000])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridNB = ParamGridBuilder()\\\n",
    "    .addGrid(nb.smoothing, [0.001, 0.01, 0.1, 0.5, 1, 1.5, 1.75])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGidLogReg = ParamGridBuilder()\\\n",
    "    .addGrid(lr.maxIter = [])\\\n",
    "    .addGrid(lr.regParam = [])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridRF = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees = [])\\\n",
    "    .addGrid(rf.maxDepth = [])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvTFIDF = CrossValidator(estimator = pipelineNB,\n",
    "                              estimatorParamMaps = paramGridTFIDF,\n",
    "                              evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\"),\n",
    "                              numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvNB = CrossValidator(estimator = pipelineNB,\n",
    "                         estimatorParamMaps = paramGridNB,\n",
    "                         evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\"),\n",
    "                         numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvLogReg = CrossValidator(estimator = pipelineLogReg,\n",
    "                         estimatorParamMaps = paramGridLogReg,\n",
    "                         evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\"),\n",
    "                         numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvRF = CrossValidator(estimator = pipelineRF,\n",
    "                         estimatorParamMaps = paramGridRF,\n",
    "                         evaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\"),\n",
    "                         numFolds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "- Unbalanced\n",
    "- No subset/sample of data\n",
    "- No stemming or lemmatization\n",
    "- No N-Grams\n",
    "- NO TF-IDF parameters\n",
    "- Use default Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = model_df.select('text', 'style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641356"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               style| count|\n",
      "+--------------------+------+\n",
      "|        American IPA|264697|\n",
      "|American Imperial...|188767|\n",
      "|American Pale Ale...|109490|\n",
      "|      Belgian Saison| 78402|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.groupBy('style').count().sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDat, testDat = data1.randomSplit([0.8, 0.2], seed = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfModBase = pipelineNB.fit(trainDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTfidfBase = tfidfModBase.transform(testDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6332633327575419"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predTfidfBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvTfidfBase = cvTFIDF.fit(trainDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predCvTfidfBase = cvTfidfBase.transform(testDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849851845318615"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predCvTfidfBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6488106384442446,\n",
       " 0.6488102554991351,\n",
       " 0.6502629046568504,\n",
       " 0.6689535237033796,\n",
       " 0.6736880706207696,\n",
       " 0.6659764256897451,\n",
       " 0.6777045229293912,\n",
       " 0.6815448256042428,\n",
       " 0.6685998753329425,\n",
       " 0.6814338977629593,\n",
       " 0.6852826632759372,\n",
       " 0.6696686955531148]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvTfidfBase.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='CrossValidatorModel_54ecc3b5de0e', name='seed', doc='random seed.'): 2507911880642868994,\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='estimator', doc='estimator to be cross-validated'): Pipeline_d24f79d6757b,\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000}],\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): MulticlassClassificationEvaluator_7fb45041f40d}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvTfidfBase.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best TF-IDF Parameters for Unbalanced Dataset\n",
    "1. numFeatures = 100000, minDocFreq = 1000\n",
    "2. numFeatures = 100000, minDocFreq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Data Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSampled = data1.sampleBy(\"style\", fractions={\"American IPA\": 75000/264697,\n",
    "                                                \"American Imperial IPA\": 75000/188767,\n",
    "                                                \"American Pale Ale (APA)\": 75000/109490,\n",
    "                                                \"Belgian Saison\": 75000/78402}, seed = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299780"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               style|count|\n",
      "+--------------------+-----+\n",
      "|        American IPA|74787|\n",
      "|American Imperial...|74850|\n",
      "|American Pale Ale...|75127|\n",
      "|      Belgian Saison|75016|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataSampled.groupBy(\"style\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDatSample, HoldoutDatSample = dataSampled.randomSplit([0.8, 0.2], seed = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTFidfModSamp = pipelineNB.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predBaseTfidfModSamp = baseTFidfModSamp.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541101716668796"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predBaseTfidfModSamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvTfidfBaseSamp = cvTFIDF.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predCvTfidfBaseSamp = cvTfidfBaseSamp.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071848554410634"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predCvTfidfBaseSamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6743677921406365,\n",
       " 0.6744102723533023,\n",
       " 0.6551742432845589,\n",
       " 0.7032835655826926,\n",
       " 0.7077131500897303,\n",
       " 0.6599031564311474,\n",
       " 0.7095983630103647,\n",
       " 0.7082906562946909,\n",
       " 0.6603426427815116,\n",
       " 0.7107182156399433,\n",
       " 0.7085748914274623,\n",
       " 0.660564046513989]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvTfidfBaseSamp.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='CrossValidatorModel_54ecc3b5de0e', name='seed', doc='random seed.'): 2507911880642868994,\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='estimator', doc='estimator to be cross-validated'): Pipeline_d24f79d6757b,\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 1000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 10000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 50000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 100},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 1000},\n",
       "  {Param(parent='HashingTF_d6479245810a', name='numFeatures', doc='number of features.'): 100000,\n",
       "   Param(parent='IDF_c596d013591a', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering'): 10000}],\n",
       " Param(parent='CrossValidatorModel_54ecc3b5de0e', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): MulticlassClassificationEvaluator_7fb45041f40d}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvTfidfBase.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best TF-IDF Parameters\n",
    "1. numFeatures = 100000, minDocFreq = 100\n",
    "2. numFeatures = 100000, minDocFreq = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModSamp = pipelineNB.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predNbModSamp = nbModSamp.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071848554410634"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predNbModSamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logModSamp = pipelineLogReg.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLogModSamp = logModSamp.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7357661508465958"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predLogModSamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModSamp = pipelineRF.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRfModSamp = rfModSamp.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.567530442615334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predRfModSamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbCvMod = cvNB.fit(TrainDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predNbCvMod = nbCvMod.transform(HoldoutDatSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7072347800265846"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predNbCvMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7109425488303297,\n",
       " 0.7109481126714816,\n",
       " 0.7109632899735919,\n",
       " 0.7109107918503748,\n",
       " 0.7107182156399433,\n",
       " 0.7105412947559459,\n",
       " 0.7104673478633691]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbCvMod.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='CrossValidatorModel_1980b0c176e3', name='seed', doc='random seed.'): 2507911880642868994,\n",
       " Param(parent='CrossValidatorModel_1980b0c176e3', name='estimator', doc='estimator to be cross-validated'): Pipeline_94cfe90cdd3a,\n",
       " Param(parent='CrossValidatorModel_1980b0c176e3', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.001},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.01},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.1},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0.5},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.5},\n",
       "  {Param(parent='NaiveBayes_d58a45717363', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.75}],\n",
       " Param(parent='CrossValidatorModel_1980b0c176e3', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): MulticlassClassificationEvaluator_0911c9d11a66}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbCvMod.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
